{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac00cb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "In 2017, researchers at Google published a paper that proposed a novel neural network\n",
    "architecture for sequence modeling. Dubbed the Transformer, this architecture outperformed\n",
    "recurrent neural networks (RNNs) on machine translation tasks, both in terms of translation\n",
    "quality and training cost.\n",
    "In parallel, an effective transfer learning method called ULMFiT showed that training long\n",
    "short-term memory (LSTM) networks on a very large and diverse corpus could produce state\n",
    "of-the-art text classifiers with little labeled data.\n",
    "These advances were the catalysts for two of today’s most well-known transformers: the\n",
    "Generative Pretrained Transformer (GPT) and Bidirectional Encoder Representations from\n",
    "Transformers (BERT). By combining the Transformer architecture with unsupervised learning,\n",
    "these models removed the need to train task-specific architectures from scratch and broke\n",
    "almost every benchmark in NLP by a significant margin. Since the release of GPT and BERT, a\n",
    "zoo of transformer models has emerged; a timeline of the most prominent entries is shown in\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17305f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In 2017, researchers at Google published a paper that proposed a novel neural network\n",
      "architecture for sequence modeling. Dubbed the Transformer, this architecture outperformed\n",
      "recurrent neural networks (RNNs) on machine translation tasks, both in terms of translation\n",
      "quality and training cost.\n",
      "In parallel, an effective transfer learning method called ULMFiT showed that training long\n",
      "short-term memory (LSTM) networks on a very large and diverse corpus could produce state\n",
      "of-the-art text classifiers with little labeled data.\n",
      "These advances were the catalysts for two of today’s most well-known \n"
     ]
    }
   ],
   "source": [
    "# Original Text Preview\n",
    "print(text[:600])   # Displaying first 600 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc63f99",
   "metadata": {},
   "source": [
    "#### String Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "650eb66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IN 2017, RESEARCHERS AT GOOGLE PUBLISHED A PAPER THAT PROPOSED A NOVEL NEURAL NETWORK\n",
      "ARCHITECTURE FOR SEQUENCE MODELING. DUBBED THE TRANSFORMER, THIS ARCHITECTURE OUTPERFORMED\n",
      "RECURRENT NEURAL NETWORKS (RNNS) ON MACHINE TRANSLATION TASKS, BOTH IN TERMS OF TRANSLATION\n",
      "QUALITY AND TRAINING COST.\n",
      "IN PARALLEL, AN EFFECTIVE TRANSFER LEARNING METHOD CALLED ULMFIT SHOWED THAT TRAINING LONG\n",
      "SHORT-TERM MEMORY (LSTM) NETWORKS ON A VERY LARGE AND DIVERSE CORPUS COULD PRODUCE STATE\n",
      "OF-THE-ART TEXT CLASSIFIERS WITH LITTLE LABELED DATA.\n",
      "THESE ADVANCES WERE THE CATALYSTS FOR TWO OF TODAY’S MOST WELL-KNOWN TRANSFORMERS: THE\n",
      "GENERATIVE PRETRAINED TRANSFORMER (GPT) AND BIDIRECTIONAL ENCODER REPRESENTATIONS FROM\n",
      "TRANSFORMERS (BERT). BY COMBINING THE TRANSFORMER ARCHITECTURE WITH UNSUPERVISED LEARNING,\n",
      "THESE MODELS REMOVED THE NEED TO TRAIN TASK-SPECIFIC ARCHITECTURES FROM SCRATCH AND BROKE\n",
      "ALMOST EVERY BENCHMARK IN NLP BY A SIGNIFICANT MARGIN. SINCE THE RELEASE OF GPT AND BERT, A\n",
      "ZOO OF TRANSFORMER MODELS HAS EMERGED; A TIMELINE OF THE MOST PROMINENT ENTRIES IS SHOWN IN\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the entire text to uppercase\n",
    "text_in_uppercase = text.upper()  # Original text converted into all uppercase characters \n",
    "print(text_in_uppercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20d75132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "in 2017, researchers at google published a paper that proposed a novel neural network\n",
      "architecture for sequence modeling. dubbed the transformer, this architecture outperformed\n",
      "recurrent neural networks (rnns) on machine translation tasks, both in terms of translation\n",
      "quality and training cost.\n",
      "in parallel, an effective transfer learning method called ulmfit showed that training long\n",
      "short-term memory (lstm) networks on a very large and diverse corpus could produce state\n",
      "of-the-art text classifiers with little labeled data.\n",
      "these advances were the catalysts for two of today’s most well-known transformers: the\n",
      "generative pretrained transformer (gpt) and bidirectional encoder representations from\n",
      "transformers (bert). by combining the transformer architecture with unsupervised learning,\n",
      "these models removed the need to train task-specific architectures from scratch and broke\n",
      "almost every benchmark in nlp by a significant margin. since the release of gpt and bert, a\n",
      "zoo of transformer models has emerged; a timeline of the most prominent entries is shown in\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the entire text to lowercase\n",
    "text_in_lowercase = text_in_uppercase.lower()   # Uppercase text converted into all lowercase characters\n",
    "print(text_in_lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "204440bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In 2017, Researchers At Google Published A Paper That Proposed A Novel Neural Network\n",
      "Architecture For Sequence Modeling. Dubbed The Transformer, This Architecture Outperformed\n",
      "Recurrent Neural Networks (Rnns) On Machine Translation Tasks, Both In Terms Of Translation\n",
      "Quality And Training Cost.\n",
      "In Parallel, An Effective Transfer Learning Method Called Ulmfit Showed That Training Long\n",
      "Short-Term Memory (Lstm) Networks On A Very Large And Diverse Corpus Could Produce State\n",
      "Of-The-Art Text Classifiers With Little Labeled Data.\n",
      "These Advances Were The Catalysts For Two Of Today’S Most Well-Known Transformers: The\n",
      "Generative Pretrained Transformer (Gpt) And Bidirectional Encoder Representations From\n",
      "Transformers (Bert). By Combining The Transformer Architecture With Unsupervised Learning,\n",
      "These Models Removed The Need To Train Task-Specific Architectures From Scratch And Broke\n",
      "Almost Every Benchmark In Nlp By A Significant Margin. Since The Release Of Gpt And Bert, A\n",
      "Zoo Of Transformer Models Has Emerged; A Timeline Of The Most Prominent Entries Is Shown In\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Capitalize every first character of each word\n",
    "titled_text = text_in_lowercase.title()    # First character of each word capitalized\n",
    "print(titled_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4decfb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "in 2017, researchers at google published a paper that proposed a novel neural network\n",
      "architecture for sequence modeling. dubbed the transformer, this architecture outperformed\n",
      "recurrent neural networks (rnns) on machine translation tasks, both in terms of translation\n",
      "quality and training cost.\n",
      "in parallel, an effective transfer learning method called ulmfit showed that training long\n",
      "short-term memory (lstm) networks on a very large and diverse corpus could produce state\n",
      "of-the-art text classifiers with little labeled data.\n",
      "these advances were the catalysts for two of today’s most well-known transformers: the\n",
      "generative pretrained transformer (gpt) and bidirectional encoder representations from\n",
      "transformers (bert). by combining the transformer architecture with unsupervised learning,\n",
      "these models removed the need to train task-specific architectures from scratch and broke\n",
      "almost every benchmark in nlp by a significant margin. since the release of gpt and bert, a\n",
      "zoo of transformer models has emerged; a timeline of the most prominent entries is shown in\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Capitalize only the first character of the entire string\n",
    "capitalized_text = text.capitalize()  # Only the first character of the first word capitalized\n",
    "print(capitalized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "baafc77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Transformer' appears 4 times\n"
     ]
    }
   ],
   "source": [
    "# Count the occurrence of a specific word\n",
    "word_count = text.count('Transformer')  # Count how many times 'Transformer' appears\n",
    "print(f\"'Transformer' appears {word_count} times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "357ffb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Transformer' first appears at index 134\n"
     ]
    }
   ],
   "source": [
    "# Find the first index of a word\n",
    "first_index = text.find('Transformer')  # Return the index of first occurrence of 'Transformer'\n",
    "print(f\"'Transformer' first appears at index {first_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a6c1094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In 2017, researchers at Google published a paper that proposed a novel neural network\n",
      "architecture for sequence modeling. Dubbed the Transformers, this architecture outperformed\n",
      "recurrent neural networks (RNNs) on machine translation tasks, both in terms of translation\n",
      "quality and training cost.\n",
      "In parallel, an effective transfer learning method called ULMFiT showed that training long\n",
      "short-term memory (LSTM) networks on a very large and diverse corpus could produce state\n",
      "of-the-art text classifiers with little labeled data.\n",
      "These advances were the catalysts for two of today’s most well-known transformers: the\n",
      "Generative Pretrained Transformers (GPT) and Bidirectional Encoder Representations from\n",
      "Transformerss (BERT). By combining the Transformers architecture with unsupervised learning,\n",
      "these models removed the need to train task-specific architectures from scratch and broke\n",
      "almost every benchmark in NLP by a significant margin. Since the release of GPT and BERT, a\n",
      "zoo of transformer models has emerged; a timeline of the most prominent entries is shown in\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace a word with another word\n",
    "replaced_text = text.replace('Transformer', 'Transformers')  # Replacing 'Transformer' with 'Transformers'\n",
    "print(replaced_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "38f171a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 2017, researchers at Google published a paper that proposed a novel neural network\n",
      "architecture for sequence modeling. Dubbed the Transformer, this architecture outperformed\n",
      "recurrent neural networks (RNNs) on machine translation tasks, both in terms of translation\n",
      "quality and training cost.\n",
      "In parallel, an effective transfer learning method called ULMFiT showed that training long\n",
      "short-term memory (LSTM) networks on a very large and diverse corpus could produce state\n",
      "of-the-art text classifiers with little labeled data.\n",
      "These advances were the catalysts for two of today’s most well-known transformers: the\n",
      "Generative Pretrained Transformer (GPT) and Bidirectional Encoder Representations from\n",
      "Transformers (BERT). By combining the Transformer architecture with unsupervised learning,\n",
      "these models removed the need to train task-specific architectures from scratch and broke\n",
      "almost every benchmark in NLP by a significant margin. Since the release of GPT and BERT, a\n",
      "zoo of transformer models has emerged; a timeline of the most prominent entries is shown in\n"
     ]
    }
   ],
   "source": [
    "# Remove whitespaces from the beginning and end\n",
    "stripped_text = text.strip()  # Remove leading and trailing whitespace\n",
    "print(stripped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed8536d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text starts with 'In 2017': False\n"
     ]
    }
   ],
   "source": [
    "# Check if text starts with a specific word\n",
    "starts_with = text.startswith('In 2017')  # Check if text starts with 'In 2017'\n",
    "print(f\"Text starts with 'In 2017': {starts_with}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3396d06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text ends with 'in': True\n"
     ]
    }
   ],
   "source": [
    "# Check if text ends with a specific word\n",
    "ends_with = text.endswith('in\\n')  # Check if text ends with 'in'\n",
    "print(f\"Text ends with 'in': {ends_with}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aed0a40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', '2017,', 'researchers', 'at', 'Google', 'published', 'a', 'paper', 'that', 'proposed']\n"
     ]
    }
   ],
   "source": [
    "# Split text into a list of words\n",
    "split_text = text.split()  # Split the text into list of words using spaces\n",
    "print(split_text[:10])  # Print first 10 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6add5b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 2017, researchers at Google published a paper that proposed a novel neural network architecture for sequence modeling. Dubbed the Transformer, this architecture outperformed recurrent neural networks (RNNs) on machine translation tasks, both in terms of translation quality and training cost. In p\n"
     ]
    }
   ],
   "source": [
    "# Join a list of words into a string\n",
    "joined_text = ' '.join(split_text)  # Join the list back into a single string\n",
    "print(joined_text[:300])  # Display first 300 characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e23f819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is '2025' all digits? True\n"
     ]
    }
   ],
   "source": [
    "# Check if all characters are digits\n",
    "digit_check = \"2025\".isdigit()  # Returns True if all characters are digits\n",
    "print(f\"Is '2025' all digits? {digit_check}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9d765337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is 'Google' all alphabetic? True\n"
     ]
    }
   ],
   "source": [
    "# Check if all characters are alphabetic\n",
    "alpha_check = \"Google\".isalpha()  # Returns True if all characters are letters\n",
    "print(f\"Is 'Google' all alphabetic? {alpha_check}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7ca5abcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is 'hello' all lowercase? True\n"
     ]
    }
   ],
   "source": [
    "# Check if all characters are lowercase\n",
    "lower_check = \"hello\".islower()  # Returns True if all characters are lowercase\n",
    "print(f\"Is 'hello' all lowercase? {lower_check}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ab69cb05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is 'HELLO' all uppercase? True\n"
     ]
    }
   ],
   "source": [
    "# Check if all characters are uppercase\n",
    "upper_check = \"HELLO\".isupper()  # Returns True if all characters are uppercase\n",
    "print(f\"Is 'HELLO' all uppercase? {upper_check}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "243ca3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is text title-cased? True\n"
     ]
    }
   ],
   "source": [
    "# Check if string is title-cased\n",
    "title_check = titled_text.istitle()  # Returns True if all words start with uppercase\n",
    "print(f\"Is text title-cased? {title_check}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a7f5a3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is 'GPT3' alphanumeric? True\n"
     ]
    }
   ],
   "source": [
    "# Check if string is alphanumeric (letters + numbers only)\n",
    "alnum_check = \"GPT3\".isalnum()  # True if string contains only letters and numbers\n",
    "print(f\"Is 'GPT3' alphanumeric? {alnum_check}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c14a0f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------NLP---------\n"
     ]
    }
   ],
   "source": [
    "# Center the text with padding\n",
    "centered = \"NLP\".center(20, '-')  # Center the string 'NLP' with '-' padding\n",
    "print(centered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7328f82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer*********\n"
     ]
    }
   ],
   "source": [
    "# Left justify the string\n",
    "left_justified = \"Transformer\".ljust(20, '*')  # Left justify with '*'\n",
    "print(left_justified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "80e63953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************BERT\n"
     ]
    }
   ],
   "source": [
    "# Right justify the string\n",
    "right_justified = \"BERT\".rjust(20, '*')  # Right justify with '*'\n",
    "print(right_justified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bdad3894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dEEP lEARNING\n"
     ]
    }
   ],
   "source": [
    "# Swap case of each letter\n",
    "swapped_case = \"Deep Learning\".swapcase()  # Lowercase becomes uppercase and vice versa\n",
    "print(swapped_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4dbf8d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\nIn 2017, researchers at Google published a paper that propo'\n"
     ]
    }
   ],
   "source": [
    "# Encode to bytes\n",
    "encoded_text = text.encode('utf-8')  # Encode text to bytes\n",
    "print(encoded_text[:60])  # Show part of the encoded byte string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5e30a84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is 'transformer_1' a valid identifier? True\n"
     ]
    }
   ],
   "source": [
    "# Check if text is a valid identifier (like variable name)\n",
    "identifier_check = \"transformer_1\".isidentifier()  # True if string is a valid variable name\n",
    "print(f\"Is 'transformer_1' a valid identifier? {identifier_check}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86776759",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
